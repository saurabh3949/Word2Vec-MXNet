{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\n",
    "import mxnet as mx\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "neg = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.00090694427\n"
     ]
    }
   ],
   "source": [
    "current = time.time()\n",
    "it = mx.io.Word2VecIter(file_path=\"./text8\",\n",
    "                        vocab_path=\"./word2vec_vocab\" # To save the vocabulary\n",
    "                        batch_size=batch_size,\n",
    "                        prefetch_buffer=100,\n",
    "                        negative_samples=neg)\n",
    "print(time.time() - current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sym_makeloss(vocab_size, dim, batch_size, neg):\n",
    "    labels = mx.sym.one_hot(mx.sym.zeros(batch_size,), depth = neg+1) #1 positive and k \"0\" labels\n",
    "    center_word = mx.sym.Variable('data')\n",
    "    target_words = mx.sym.Variable('softmax_label') # 1 target + k negative samples\n",
    "    center_vector = mx.sym.Embedding(data = center_word, input_dim = vocab_size,\n",
    "                                  output_dim = dim, name = 'syn0_embedding')\n",
    "    target_vectors = mx.sym.Embedding(data = target_words, input_dim = vocab_size,\n",
    "                                   output_dim = dim, name = 'syn1_embedding')\n",
    "    pred = mx.sym.batch_dot(target_vectors, center_vector, transpose_b=True)\n",
    "    sigmoid = mx.sym.sigmoid(mx.sym.flatten(pred))\n",
    "    loss = -1 * mx.sym.sum(labels * mx.sym.log(sigmoid) + (1 - labels) * mx.sym.log(1 - sigmoid), axis=1)\n",
    "    loss_layer = mx.sym.MakeLoss(loss)\n",
    "    return loss_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the list of words from the binary file that has null terminated strings.\n",
    "# This file is produced by the data iterator after doing the preprocessing in C++\n",
    "def read_binary(f, bufsize):\n",
    "    buf = \"\"\n",
    "    data = True\n",
    "    while data:\n",
    "        data = f.read(bufsize)\n",
    "        buf += data\n",
    "        lines = buf.split('\\x00')\n",
    "        buf = lines.pop()\n",
    "        for line in lines:\n",
    "            yield line\n",
    "    yield buf\n",
    "    \n",
    "def mean_loss(label, pred):\n",
    "    return np.mean(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin = open(\"./word2vec_vocab\", \"rb\") # Iterator saves this automatically\n",
    "words_list = read_binary(fin, 1024*1024)\n",
    "vocab = [word for word in words_list]\n",
    "word_to_index = {vocab[i]:i for i in range(len(vocab))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym = get_sym_makeloss(vocab_size, vector_dim, batch_size, neg)\n",
    "network = mx.mod.Module(sym ,context=mx.gpu())\n",
    "network.bind(data_shapes=it.provide_data, label_shapes=it.provide_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = mx.optimizer.Adam(learning_rate=.001, rescale_grad=1.0/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Already bound, ignoring bind()\n",
      "INFO:Epoch[0] Batch [1000]\tSpeed: 216630.06 samples/sec\tmean_loss=3.182736\n",
      "INFO:Epoch[0] Batch [2000]\tSpeed: 218407.69 samples/sec\tmean_loss=2.276461\n",
      "INFO:Epoch[0] Batch [3000]\tSpeed: 220015.31 samples/sec\tmean_loss=2.253708\n",
      "INFO:Epoch[0] Batch [4000]\tSpeed: 222719.81 samples/sec\tmean_loss=2.172011\n",
      "INFO:Epoch[0] Batch [5000]\tSpeed: 221057.09 samples/sec\tmean_loss=2.090425\n",
      "INFO:Epoch[0] Batch [6000]\tSpeed: 231495.19 samples/sec\tmean_loss=2.189624\n",
      "INFO:Epoch[0] Batch [7000]\tSpeed: 228004.42 samples/sec\tmean_loss=2.076356\n",
      "INFO:Epoch[0] Batch [8000]\tSpeed: 227395.95 samples/sec\tmean_loss=2.116351\n",
      "INFO:Epoch[0] Batch [9000]\tSpeed: 227821.90 samples/sec\tmean_loss=2.084706\n",
      "INFO:Epoch[0] Batch [10000]\tSpeed: 222348.91 samples/sec\tmean_loss=2.013445\n",
      "INFO:Epoch[0] Batch [11000]\tSpeed: 218110.94 samples/sec\tmean_loss=2.156342\n",
      "INFO:Epoch[0] Batch [12000]\tSpeed: 224086.13 samples/sec\tmean_loss=2.040256\n",
      "INFO:Epoch[0] Batch [13000]\tSpeed: 215975.93 samples/sec\tmean_loss=2.081876\n",
      "INFO:Epoch[0] Batch [14000]\tSpeed: 223211.34 samples/sec\tmean_loss=2.063716\n",
      "INFO:Epoch[0] Batch [15000]\tSpeed: 219123.44 samples/sec\tmean_loss=1.991266\n",
      "INFO:Epoch[0] Batch [16000]\tSpeed: 225015.97 samples/sec\tmean_loss=2.140804\n",
      "INFO:Epoch[0] Batch [17000]\tSpeed: 217679.88 samples/sec\tmean_loss=2.040144\n",
      "INFO:Epoch[0] Batch [18000]\tSpeed: 226393.41 samples/sec\tmean_loss=2.048723\n",
      "INFO:Epoch[0] Batch [19000]\tSpeed: 227344.13 samples/sec\tmean_loss=2.030444\n",
      "INFO:Epoch[0] Batch [20000]\tSpeed: 221361.97 samples/sec\tmean_loss=1.954846\n",
      "INFO:Epoch[0] Batch [21000]\tSpeed: 223903.49 samples/sec\tmean_loss=2.086621\n",
      "INFO:Epoch[0] Batch [22000]\tSpeed: 220753.43 samples/sec\tmean_loss=2.017514\n",
      "INFO:Epoch[0] Batch [23000]\tSpeed: 224974.23 samples/sec\tmean_loss=2.026557\n",
      "INFO:Epoch[0] Batch [24000]\tSpeed: 225014.76 samples/sec\tmean_loss=2.066373\n",
      "INFO:Epoch[0] Batch [25000]\tSpeed: 221482.20 samples/sec\tmean_loss=1.987588\n",
      "INFO:Epoch[0] Batch [26000]\tSpeed: 233582.96 samples/sec\tmean_loss=2.097982\n",
      "INFO:Epoch[0] Batch [27000]\tSpeed: 223752.16 samples/sec\tmean_loss=2.046453\n",
      "INFO:Epoch[0] Train-mean_loss=2.007799\n",
      "INFO:Epoch[0] Time cost=255.937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.949116945\n"
     ]
    }
   ],
   "source": [
    "current_time = time.time()\n",
    "network.fit(it, num_epoch=1,optimizer=opt,\n",
    "            eval_metric=mx.metric.CustomMetric(mean_loss),\n",
    "            batch_end_callback=mx.callback.Speedometer(batch_size, 1000),\n",
    "            initializer=mx.initializer.Uniform(scale=.05))\n",
    "print time.time() - current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "all_vecs = network.get_params()[0][\"syn0_embedding_weight\"].asnumpy()\n",
    "all_vecs = normalize(all_vecs, copy=False)\n",
    "\n",
    "def find_most_similar(word, vocab, word_to_index):\n",
    "    if word not in vocab:\n",
    "        print(\"Sorry word not found. Please try another one.\")\n",
    "    else:  \n",
    "        i1 = word_to_index[word]\n",
    "        prod = all_vecs.dot(all_vecs[i1])\n",
    "        i2 = (-prod).argsort()[1:10]\n",
    "        for i in i2:\n",
    "            print vocab[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cars\n",
      "driver\n",
      "motorcycle\n",
      "racing\n",
      "truck\n",
      "taxi\n",
      "motocross\n",
      "bike\n",
      "seater\n"
     ]
    }
   ],
   "source": [
    "find_most_similar(\"car\", vocab, word_to_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
