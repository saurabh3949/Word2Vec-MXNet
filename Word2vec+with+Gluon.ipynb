{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec using MXNet Gluon API\n",
    "\n",
    "The goal of this notebook is to show Word2Vec Skipgram implementation with Negative Sampling to train word vectors on the text8 dataset.    \n",
    "\n",
    "Please note that python based deep learning framworks are not suitable for Word2Vec. This is maily due to python's limitations that force the data iterator to be single threaded and does not allow asynchronous SGD - all CPU cores cannot do the optimization in parallel to each other as done in the original Word2Vec C implementation.    \n",
    "\n",
    "This notebook is for demo purposes only - highlighting the features of new Gluon API that makes it very easy to prototype complex models with custom losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys, random, time, math\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import Block, nn, autograd\n",
    "import cPickle\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the text8 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-08-08 23:02:12--  http://mattmahoney.net/dc/text8.zip\n",
      "Resolving mattmahoney.net (mattmahoney.net)... 98.139.135.129\n",
      "Connecting to mattmahoney.net (mattmahoney.net)|98.139.135.129|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 31344016 (30M) [application/zip]\n",
      "Saving to: ‘text8.gz’\n",
      "\n",
      "text8.gz            100%[===================>]  29.89M  2.47MB/s    in 12s     \n",
      "\n",
      "2017-08-08 23:02:25 (2.46 MB/s) - ‘text8.gz’ saved [31344016/31344016]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://mattmahoney.net/dc/text8.zip -O text8.gz && gzip -d text8.gz -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the text8 file to build vocabulary, word-to-word_index and word_index-to-word mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buf = open(\"text8\").read()\n",
    "tks = buf.split(' ')\n",
    "vocab = {}\n",
    "wid_to_word = [\"NA\"]\n",
    "freq = [0] # Store frequency of all tokens.\n",
    "data = [] # Store word indices\n",
    "for tk in tks:\n",
    "    if len(tk) == 0:\n",
    "        continue\n",
    "    if tk not in vocab:\n",
    "        vocab[tk] = len(vocab) + 1\n",
    "        freq.append(0)\n",
    "        wid_to_word.append(tk)\n",
    "    wid = vocab[tk]\n",
    "    data.append(wid)\n",
    "    freq[wid] += 1\n",
    "negative = [] # Build this table for negative sampling for words from a Unigram distribution.\n",
    "for i, v in enumerate(freq):\n",
    "    if i == 0 or v < 5:\n",
    "        continue\n",
    "    v = int(math.pow(v * 1.0, 0.75))\n",
    "    negative += [i for _ in range(v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the constrants / hyperparameters. Set the context to GPU **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(wid_to_word)\n",
    "BATCH_SIZE = 512\n",
    "WORD_DIM = 100\n",
    "NEGATIVE_SAMPLES = 5\n",
    "\n",
    "# Preferably use GPU for faster training.\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataBatch(object):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Avoid running the cell below this comment for demo. It takes around 20 min to generate training data for 1 epoch using Python. This is one of the primary reasons why Word2Vec is not suitable for Python based deep learning frameworks. **  \n",
    "We have already generated the training dataset and uploaded it to S3. Please use the next cell to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataIterator(mx.io.DataIter):\n",
    "    def __init__(self,batch_size=512, negative_samples=5, window=5):\n",
    "        super(Word2VecDataIterator, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.negative_samples = negative_samples\n",
    "        self.window = 5\n",
    "        self.data, self.negative, self.vocab, self.freq = (data, negative, vocab, freq)\n",
    "\n",
    "    @property\n",
    "    def provide_data(self):\n",
    "        return [('contexts', (self.batch_size, 1))]\n",
    "\n",
    "    @property\n",
    "    def provide_label(self):\n",
    "        return  [('targets', (self.batch_size, self.negative + 1))]\n",
    "\n",
    "    def sample_ne(self):\n",
    "        return self.negative[random.randint(0, len(self.negative) - 1)]\n",
    "\n",
    "    def __iter__(self):\n",
    "        center_data = []\n",
    "        targets = []\n",
    "        result = 0\n",
    "        for pos, word in enumerate(self.data):\n",
    "            boundary = random.randint(1,self.window)  # `b` in the original word2vec code\n",
    "            for index in range(-boundary, boundary+1):\n",
    "                if (index != 0 and pos + boundary >= 0 and pos + boundary < len(self.data)):\n",
    "                    center_word = word\n",
    "                    context_word = self.data[pos + index]\n",
    "                    if center_word != context_word:\n",
    "                        targets_vec = []\n",
    "                        center_data.append([word])\n",
    "                        targets_vec.append(context_word)\n",
    "                        while len(targets_vec) < self.negative_samples + 1:\n",
    "                            w = self.sample_ne()\n",
    "                            if w != word:\n",
    "                                targets_vec.append(w)\n",
    "                        targets.append(targets_vec)\n",
    "\n",
    "            # Check if batch size is full\n",
    "            if len(center_data) > self.batch_size:\n",
    "                data_all = [mx.nd.array(center_data[:self.batch_size])]\n",
    "                label_all = [mx.nd.array(targets[:self.batch_size])]\n",
    "                yield DataBatch(data_all, label_all)\n",
    "                center_data = center_data[self.batch_size:]\n",
    "                targets = targets[self.batch_size:]\n",
    "\n",
    "data_iterator = Word2VecDataIterator(batch_size=BATCH_SIZE,\n",
    "                                     negative_samples=NEGATIVE_SAMPLES,\n",
    "                                     window=5)   \n",
    "all_batches = []\n",
    "for batch in data_iterator:\n",
    "    all_batches.append(batch)\n",
    "cPickle.dump(all_data, open('all_batches.p', 'wb')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Download and read the pickled training dataset **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!https://s3-us-west-2.amazonaws.com/gsaur-dev/input.p\n",
    "all_batches = cPickle.load(open('input.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec training\n",
    "Word2vec represents each word $w$ in a vocabulary $V$ of size $T$ as a low-dimensional dense vector $v_w$ in an embedding space $\\mathbb{R}^D$. It attempts to learn the continuous word vectors $v_w$, $\\forall w \\in V$ , from a training corpus such that the spatial distance between words then describes the similarity between words, e.g., the closer two words are in the embedding space, the more similar they are semantically and syntactically.  \n",
    "\n",
    "The skipgram architecture tries to predict the context given a word. The problem of predicting context words is framed as a set of independent binary classification tasks. Then the goal is to independently predict the presence (or absence) of context words. For the word at position $t$ we consider all context words as positive examples and sample negatives at random from the dictionary. For a chosen context position $c$, using the binary logistic loss, we obtain the following negative log-likelihood:\n",
    "\n",
    "$$ \\log (1 + e^{-s(w_t, w_c)}) +  \\sum_{n \\in \\mathcal{N}_{t,c}}^{}{\\log (1 + e^{s(w_t, n)})}$$\n",
    "\n",
    "where $w_t$ is a center word, $w_c$ is a context word, $\\mathcal{N}_{t,c}$ is a set of negative examples sampled from the vocabulary. By denoting the logistic loss function $l : x \\mapsto \\log(1 + e^{-x})$, we can re-write the objective as:\n",
    "\n",
    "$$ \\sum_{t=1}^{T}{ \\sum_{c \\in C_t}^{}{ \\big[ l(s(w_t, w_c))} + \\sum_{n \\in \\mathcal{N}_{t,c}}^{}{l(-s(w_t, n))}   \\big]} $$\n",
    "\n",
    "where $s(w_t, w_c) = u_{w_t}^T v_{w_c}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(gluon.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Model, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            \n",
    "            # Embedding for input words with dimensions VOCAB_SIZE X WORD_DIM\n",
    "            self.center = nn.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                       output_dim=WORD_DIM,\n",
    "                                       weight_initializer=mx.initializer.Uniform(1.0/WORD_DIM))\n",
    "            \n",
    "            # Embedding for output words with dimensions VOCAB_SIZE X WORD_DIM\n",
    "            self.target = nn.Embedding(input_dim=VOCAB_SIZE,\n",
    "                                       output_dim=WORD_DIM,\n",
    "                                       weight_initializer=mx.initializer.Zero())\n",
    "\n",
    "    def hybrid_forward(self, F, center, targets, labels):\n",
    "        \"\"\"\n",
    "        Returns the word2vec skipgram with negative sampling network.\n",
    "        :param F: F is a function space that depends on the type of other inputs.\n",
    "                  If their type is NDArray, then F will be mxnet.nd otherwise it will be mxnet.sym\n",
    "        :param center: A symbol/NDArray with dimensions (batch_size, 1). Contains the index of center word\n",
    "                       for each batch.\n",
    "        :param targets: A symbol/NDArray with dimensions (batch_size, negative_samples + 1). \n",
    "                        Contains the indices of 1 target word and `n` negative samples (n=5 in this example)\n",
    "        :param labels: A symbol/NDArray with dimensions (batch_size, negative_samples + 1). \n",
    "                        For 5 negative samples, the array for each batch is [1,0,0,0,0,0] i.e. label is 1\n",
    "                        for target word and 0 for negative samples\n",
    "        :return: Return a HybridBlock object\n",
    "        \"\"\"\n",
    "        center_vector = self.center(center)\n",
    "        target_vectors = self.target(targets)\n",
    "        pred = F.broadcast_mul(center_vector, target_vectors)\n",
    "        pred = F.sum(data = pred, axis = 2)\n",
    "        sigmoid = F.sigmoid(pred)\n",
    "        loss = F.sum(labels * F.log(sigmoid) + (1 - labels) * F.log(1 - sigmoid), axis=1)\n",
    "        loss = loss * -1.0 / BATCH_SIZE\n",
    "        loss_layer = F.MakeLoss(loss)\n",
    "        return loss_layer\n",
    "\n",
    "model = Model()\n",
    "model.initialize(ctx=ctx)\n",
    "model.hybridize() # Convert to a symbolic network for efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Use a large learning rate since batch size is large - 512 in this example. In the original word2vec C implementation, stochastic gradient descent is used i.e. batch_size = 1. However, batch_size = 1 for deep learning frameworks slows down the training drastically, therefore a larger batch size is used. Larger the batch size, faster the training and slower the convergence. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'sgd', {'learning_rate':4,'clip_gradient':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 49. Moving avg of loss: 4.13644049418\n",
      "Epoch 0, batch 99. Moving avg of loss: 4.00840381263\n",
      "Epoch 0, batch 149. Moving avg of loss: 3.8647550546\n",
      "Epoch 0, batch 199. Moving avg of loss: 3.71921361786\n",
      "Epoch 0, batch 249. Moving avg of loss: 3.60396338842\n",
      "Epoch 0, batch 299. Moving avg of loss: 3.50200666046\n",
      "Epoch 0, batch 349. Moving avg of loss: 3.44372812836\n",
      "Epoch 0, batch 399. Moving avg of loss: 3.37975349744\n",
      "Epoch 0, batch 449. Moving avg of loss: 3.30362816408\n",
      "Epoch 0, batch 499. Moving avg of loss: 3.25998733948\n",
      "Epoch 0, batch 549. Moving avg of loss: 3.21276051647\n",
      "Epoch 0, batch 599. Moving avg of loss: 3.18018420469\n",
      "Epoch 0, batch 649. Moving avg of loss: 3.16044233008\n",
      "Epoch 0, batch 699. Moving avg of loss: 3.14746064421\n",
      "Epoch 0, batch 749. Moving avg of loss: 3.13331177527\n",
      "Epoch 0, batch 799. Moving avg of loss: 3.11470255439\n",
      "Epoch 0, batch 849. Moving avg of loss: 3.11446874172\n",
      "Epoch 0, batch 899. Moving avg of loss: 3.09167773878\n",
      "Epoch 0, batch 949. Moving avg of loss: 3.06617585747\n",
      "Epoch 0, batch 999. Moving avg of loss: 3.05154607231\n",
      "Epoch 0, batch 1049. Moving avg of loss: 3.06433665287\n",
      "Epoch 0, batch 1099. Moving avg of loss: 3.06745624566\n",
      "Epoch 0, batch 1149. Moving avg of loss: 3.04007462818\n",
      "Epoch 0, batch 1199. Moving avg of loss: 3.02778670239\n",
      "Epoch 0, batch 1249. Moving avg of loss: 3.03964380455\n",
      "Epoch 0, batch 1299. Moving avg of loss: 3.02405016577\n",
      "Epoch 0, batch 1349. Moving avg of loss: 3.01043800416\n",
      "Epoch 0, batch 1399. Moving avg of loss: 3.00508516463\n",
      "Epoch 0, batch 1449. Moving avg of loss: 2.99648759431\n",
      "Epoch 0, batch 1499. Moving avg of loss: 2.99297712551\n",
      "Epoch 0, batch 1549. Moving avg of loss: 2.98561480891\n",
      "Epoch 0, batch 1599. Moving avg of loss: 2.98482443179\n",
      "Epoch 0, batch 1649. Moving avg of loss: 2.97710555955\n",
      "Epoch 0, batch 1699. Moving avg of loss: 2.96041979871\n",
      "Epoch 0, batch 1749. Moving avg of loss: 2.95293687328\n",
      "Epoch 0, batch 1799. Moving avg of loss: 2.95014147815\n",
      "Epoch 0, batch 1849. Moving avg of loss: 2.94870105901\n",
      "Epoch 0, batch 1899. Moving avg of loss: 2.94538858075\n",
      "Epoch 0, batch 1949. Moving avg of loss: 2.93867705142\n",
      "Epoch 0, batch 1999. Moving avg of loss: 2.94193905794\n",
      "Epoch 0, batch 2049. Moving avg of loss: 2.94734602163\n",
      "Epoch 0, batch 2099. Moving avg of loss: 2.9501553251\n",
      "Epoch 0, batch 2149. Moving avg of loss: 2.95377499493\n",
      "Epoch 0, batch 2199. Moving avg of loss: 2.94596291634\n",
      "Epoch 0, batch 2249. Moving avg of loss: 2.93547747922\n",
      "Epoch 0, batch 2299. Moving avg of loss: 2.9413488133\n",
      "Epoch 0, batch 2349. Moving avg of loss: 2.93512274237\n",
      "Epoch 0, batch 2399. Moving avg of loss: 2.9435153319\n",
      "Epoch 0, batch 2449. Moving avg of loss: 2.94682243362\n",
      "Epoch 0, batch 2499. Moving avg of loss: 2.92745837251\n",
      "Epoch 0, batch 2549. Moving avg of loss: 2.91933057274\n",
      "Epoch 0, batch 2599. Moving avg of loss: 2.91249909972\n",
      "Epoch 0, batch 2649. Moving avg of loss: 2.90427488958\n",
      "Epoch 0, batch 2699. Moving avg of loss: 2.90993143212\n",
      "Epoch 0, batch 2749. Moving avg of loss: 2.90798908372\n",
      "Epoch 0, batch 2799. Moving avg of loss: 2.88615057089\n",
      "Epoch 0, batch 2849. Moving avg of loss: 2.88409088716\n",
      "Epoch 0, batch 2899. Moving avg of loss: 2.88980612053\n",
      "Epoch 0, batch 2949. Moving avg of loss: 2.89494710929\n",
      "Epoch 0, batch 2999. Moving avg of loss: 2.89429695225\n",
      "Epoch 0, batch 3049. Moving avg of loss: 2.8851669809\n",
      "Epoch 0, batch 3099. Moving avg of loss: 2.88890987352\n",
      "Epoch 0, batch 3149. Moving avg of loss: 2.88308522272\n",
      "Epoch 0, batch 3199. Moving avg of loss: 2.880894815\n",
      "Epoch 0, batch 3249. Moving avg of loss: 2.86936208433\n",
      "Epoch 0, batch 3299. Moving avg of loss: 2.85797085566\n",
      "Epoch 0, batch 3349. Moving avg of loss: 2.85982927043\n",
      "Epoch 0, batch 3399. Moving avg of loss: 2.86064941738\n",
      "Epoch 0, batch 3449. Moving avg of loss: 2.83780707674\n",
      "Epoch 0, batch 3499. Moving avg of loss: 2.85115494928\n",
      "Epoch 0, batch 3549. Moving avg of loss: 2.86485824469\n",
      "Epoch 0, batch 3599. Moving avg of loss: 2.87171137754\n",
      "Epoch 0, batch 3649. Moving avg of loss: 2.87016831832\n",
      "Epoch 0, batch 3699. Moving avg of loss: 2.87590745905\n",
      "Epoch 0, batch 3749. Moving avg of loss: 2.87916651416\n",
      "Epoch 0, batch 3799. Moving avg of loss: 2.8717023644\n",
      "Epoch 0, batch 3849. Moving avg of loss: 2.86363781944\n",
      "Epoch 0, batch 3899. Moving avg of loss: 2.86073020645\n",
      "Epoch 0, batch 3949. Moving avg of loss: 2.8694649315\n",
      "Epoch 0, batch 3999. Moving avg of loss: 2.86728671803\n",
      "Epoch 0, batch 4049. Moving avg of loss: 2.86143168007\n",
      "Epoch 0, batch 4099. Moving avg of loss: 2.84965441348\n",
      "Epoch 0, batch 4149. Moving avg of loss: 2.84457352341\n",
      "Epoch 0, batch 4199. Moving avg of loss: 2.84197461261\n",
      "Epoch 0, batch 4249. Moving avg of loss: 2.83965467493\n",
      "Epoch 0, batch 4299. Moving avg of loss: 2.84547503499\n",
      "Epoch 0, batch 4349. Moving avg of loss: 2.84922194317\n",
      "Epoch 0, batch 4399. Moving avg of loss: 2.84058579324\n",
      "Epoch 0, batch 4449. Moving avg of loss: 2.85970420522\n",
      "Epoch 0, batch 4499. Moving avg of loss: 2.85641687245\n",
      "Epoch 0, batch 4549. Moving avg of loss: 2.85134629578\n",
      "Epoch 0, batch 4599. Moving avg of loss: 2.85240768515\n",
      "Epoch 0, batch 4649. Moving avg of loss: 2.83647726379\n",
      "Epoch 0, batch 4699. Moving avg of loss: 2.83610231882\n",
      "Epoch 0, batch 4749. Moving avg of loss: 2.82833203149\n",
      "Epoch 0, batch 4799. Moving avg of loss: 2.83736634485\n",
      "Epoch 0, batch 4849. Moving avg of loss: 2.84359159674\n",
      "Epoch 0, batch 4899. Moving avg of loss: 2.84703876547\n",
      "Epoch 0, batch 4949. Moving avg of loss: 2.84435363326\n",
      "Epoch 0, batch 4999. Moving avg of loss: 2.8390964286\n",
      "Epoch 0, batch 5049. Moving avg of loss: 2.83807408124\n",
      "Epoch 0, batch 5099. Moving avg of loss: 2.83320629586\n",
      "Epoch 0, batch 5149. Moving avg of loss: 2.82359845204\n",
      "Epoch 0, batch 5199. Moving avg of loss: 2.81691977222\n",
      "Epoch 0, batch 5249. Moving avg of loss: 2.81595218307\n",
      "Epoch 0, batch 5299. Moving avg of loss: 2.82141443984\n",
      "Epoch 0, batch 5349. Moving avg of loss: 2.81431326525\n",
      "Epoch 0, batch 5399. Moving avg of loss: 2.82633517853\n",
      "Epoch 0, batch 5449. Moving avg of loss: 2.81868688108\n",
      "Epoch 0, batch 5499. Moving avg of loss: 2.8175352126\n",
      "Epoch 0, batch 5549. Moving avg of loss: 2.82127066511\n",
      "Epoch 0, batch 5599. Moving avg of loss: 2.82722755657\n",
      "Epoch 0, batch 5649. Moving avg of loss: 2.81751054199\n",
      "Epoch 0, batch 5699. Moving avg of loss: 2.82310612325\n",
      "Epoch 0, batch 5749. Moving avg of loss: 2.83318494413\n",
      "Epoch 0, batch 5799. Moving avg of loss: 2.82699836826\n",
      "Epoch 0, batch 5849. Moving avg of loss: 2.81472709894\n",
      "Epoch 0, batch 5899. Moving avg of loss: 2.81162200649\n",
      "Epoch 0, batch 5949. Moving avg of loss: 2.8237430385\n",
      "Epoch 0, batch 5999. Moving avg of loss: 2.81779980226\n",
      "Epoch 0, batch 6049. Moving avg of loss: 2.80467675135\n",
      "Epoch 0, batch 6099. Moving avg of loss: 2.81898760005\n",
      "Epoch 0, batch 6149. Moving avg of loss: 2.81477076049\n",
      "Epoch 0, batch 6199. Moving avg of loss: 2.80641479954\n",
      "Epoch 0, batch 6249. Moving avg of loss: 2.79083662629\n",
      "Epoch 0, batch 6299. Moving avg of loss: 2.79191786703\n",
      "Epoch 0, batch 6349. Moving avg of loss: 2.78087779924\n",
      "Epoch 0, batch 6399. Moving avg of loss: 2.77925782921\n",
      "Epoch 0, batch 6449. Moving avg of loss: 2.78977741198\n",
      "Epoch 0, batch 6499. Moving avg of loss: 2.8249789643\n",
      "Epoch 0, batch 6549. Moving avg of loss: 2.8299153896\n",
      "Epoch 0, batch 6599. Moving avg of loss: 2.82143889948\n",
      "Epoch 0, batch 6649. Moving avg of loss: 2.81642323936\n",
      "Epoch 0, batch 6699. Moving avg of loss: 2.80356175037\n",
      "Epoch 0, batch 6749. Moving avg of loss: 2.78955236363\n",
      "Epoch 0, batch 6799. Moving avg of loss: 2.78983585954\n",
      "Epoch 0, batch 6849. Moving avg of loss: 2.7969064007\n",
      "Epoch 0, batch 6899. Moving avg of loss: 2.78693170877\n",
      "Epoch 0, batch 6949. Moving avg of loss: 2.79723739356\n",
      "Epoch 0, batch 6999. Moving avg of loss: 2.80444484208\n",
      "Epoch 0, batch 7049. Moving avg of loss: 2.80798868243\n",
      "Epoch 0, batch 7099. Moving avg of loss: 2.80232935916\n",
      "Epoch 0, batch 7149. Moving avg of loss: 2.8019442585\n",
      "Epoch 0, batch 7199. Moving avg of loss: 2.81383098423\n",
      "Epoch 0, batch 7249. Moving avg of loss: 2.82071398934\n",
      "Epoch 0, batch 7299. Moving avg of loss: 2.81602220472\n",
      "Epoch 0, batch 7349. Moving avg of loss: 2.80401954223\n",
      "Epoch 0, batch 7399. Moving avg of loss: 2.80548107592\n",
      "Epoch 0, batch 7449. Moving avg of loss: 2.78910527961\n",
      "Epoch 0, batch 7499. Moving avg of loss: 2.8033451011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 7549. Moving avg of loss: 2.7966517535\n",
      "Epoch 0, batch 7599. Moving avg of loss: 2.7865592484\n",
      "Epoch 0, batch 7649. Moving avg of loss: 2.79245494198\n",
      "Epoch 0, batch 7699. Moving avg of loss: 2.79691023233\n",
      "Epoch 0, batch 7749. Moving avg of loss: 2.80492532327\n",
      "Epoch 0, batch 7799. Moving avg of loss: 2.7972106854\n",
      "Epoch 0, batch 7849. Moving avg of loss: 2.80658139456\n",
      "Epoch 0, batch 7899. Moving avg of loss: 2.80422056082\n",
      "Epoch 0, batch 7949. Moving avg of loss: 2.80966109057\n",
      "Epoch 0, batch 7999. Moving avg of loss: 2.81183956232\n",
      "Epoch 0, batch 8049. Moving avg of loss: 2.81393973531\n",
      "Epoch 0, batch 8099. Moving avg of loss: 2.79975138621\n",
      "Epoch 0, batch 8149. Moving avg of loss: 2.79057591687\n",
      "Epoch 0, batch 8199. Moving avg of loss: 2.80018016829\n",
      "Epoch 0, batch 8249. Moving avg of loss: 2.80338265379\n",
      "Epoch 0, batch 8299. Moving avg of loss: 2.80498653395\n",
      "Epoch 0, batch 8349. Moving avg of loss: 2.7975653348\n",
      "Epoch 0, batch 8399. Moving avg of loss: 2.80481106726\n",
      "Epoch 0, batch 8449. Moving avg of loss: 2.79252902169\n",
      "Epoch 0, batch 8499. Moving avg of loss: 2.78198201953\n",
      "Epoch 0, batch 8549. Moving avg of loss: 2.77876958103\n",
      "Epoch 0, batch 8599. Moving avg of loss: 2.77004791286\n",
      "Epoch 0, batch 8649. Moving avg of loss: 2.78442083264\n",
      "Epoch 0, batch 8699. Moving avg of loss: 2.79569105592\n",
      "Epoch 0, batch 8749. Moving avg of loss: 2.78248730875\n",
      "Epoch 0, batch 8799. Moving avg of loss: 2.7902145605\n",
      "Epoch 0, batch 8849. Moving avg of loss: 2.79253843323\n",
      "Epoch 0, batch 8899. Moving avg of loss: 2.77577153309\n",
      "Epoch 0, batch 8949. Moving avg of loss: 2.7777575342\n",
      "Epoch 0, batch 8999. Moving avg of loss: 2.79210886751\n",
      "Epoch 0, batch 9049. Moving avg of loss: 2.80621810005\n",
      "Epoch 0, batch 9099. Moving avg of loss: 2.7992218178\n",
      "Epoch 0, batch 9149. Moving avg of loss: 2.79713432949\n",
      "Epoch 0, batch 9199. Moving avg of loss: 2.79921675962\n",
      "Epoch 0, batch 9249. Moving avg of loss: 2.80585452527\n",
      "Epoch 0, batch 9299. Moving avg of loss: 2.79154275295\n",
      "Epoch 0, batch 9349. Moving avg of loss: 2.78071341933\n",
      "Epoch 0, batch 9399. Moving avg of loss: 2.78352467216\n",
      "Epoch 0, batch 9449. Moving avg of loss: 2.77920251435\n",
      "Epoch 0, batch 9499. Moving avg of loss: 2.78197671788\n",
      "Epoch 0, batch 9549. Moving avg of loss: 2.77932135448\n",
      "Epoch 0, batch 9599. Moving avg of loss: 2.77925139496\n",
      "Epoch 0, batch 9649. Moving avg of loss: 2.79024799496\n",
      "Epoch 0, batch 9699. Moving avg of loss: 2.79285968335\n",
      "Epoch 0, batch 9749. Moving avg of loss: 2.79239114351\n",
      "Epoch 0, batch 9799. Moving avg of loss: 2.79257357015\n",
      "Epoch 0, batch 9849. Moving avg of loss: 2.77557605519\n",
      "Epoch 0, batch 9899. Moving avg of loss: 2.78169551243\n",
      "Epoch 0, batch 9949. Moving avg of loss: 2.77638783068\n",
      "Epoch 0, batch 9999. Moving avg of loss: 2.78389798607\n",
      "Epoch 0, batch 10049. Moving avg of loss: 2.76838428245\n",
      "Epoch 0, batch 10099. Moving avg of loss: 2.75527576607\n",
      "Epoch 0, batch 10149. Moving avg of loss: 2.76517424048\n",
      "Epoch 0, batch 10199. Moving avg of loss: 2.7740854143\n",
      "Epoch 0, batch 10249. Moving avg of loss: 2.77393183402\n",
      "Epoch 0, batch 10299. Moving avg of loss: 2.75776309768\n",
      "Epoch 0, batch 10349. Moving avg of loss: 2.7712600122\n",
      "Epoch 0, batch 10399. Moving avg of loss: 2.77473915198\n",
      "Epoch 0, batch 10449. Moving avg of loss: 2.75172685029\n",
      "Epoch 0, batch 10499. Moving avg of loss: 2.76858429238\n",
      "Epoch 0, batch 10549. Moving avg of loss: 2.78048717699\n",
      "Epoch 0, batch 10599. Moving avg of loss: 2.77838176643\n",
      "Epoch 0, batch 10649. Moving avg of loss: 2.77543907305\n",
      "Epoch 0, batch 10699. Moving avg of loss: 2.75822869822\n",
      "Epoch 0, batch 10749. Moving avg of loss: 2.75520458893\n",
      "Epoch 0, batch 10799. Moving avg of loss: 2.76310009042\n",
      "Epoch 0, batch 10849. Moving avg of loss: 2.77237379393\n",
      "Epoch 0, batch 10899. Moving avg of loss: 2.7850799526\n",
      "Epoch 0, batch 10949. Moving avg of loss: 2.79132617596\n",
      "Epoch 0, batch 10999. Moving avg of loss: 2.77367819576\n",
      "Epoch 0, batch 11049. Moving avg of loss: 2.76530009048\n",
      "Epoch 0, batch 11099. Moving avg of loss: 2.75852387977\n",
      "Epoch 0, batch 11149. Moving avg of loss: 2.76252249299\n",
      "Epoch 0, batch 11199. Moving avg of loss: 2.76433409318\n",
      "Epoch 0, batch 11249. Moving avg of loss: 2.78824901825\n",
      "Epoch 0, batch 11299. Moving avg of loss: 2.79978469823\n",
      "Epoch 0, batch 11349. Moving avg of loss: 2.77069261483\n",
      "Epoch 0, batch 11399. Moving avg of loss: 2.7744209702\n",
      "Epoch 0, batch 11449. Moving avg of loss: 2.77653090111\n",
      "Epoch 0, batch 11499. Moving avg of loss: 2.7824707483\n",
      "Epoch 0, batch 11549. Moving avg of loss: 2.77981058704\n",
      "Epoch 0, batch 11599. Moving avg of loss: 2.77581345941\n",
      "Epoch 0, batch 11649. Moving avg of loss: 2.7842808319\n",
      "Epoch 0, batch 11699. Moving avg of loss: 2.76086157259\n",
      "Epoch 0, batch 11749. Moving avg of loss: 2.79383295984\n",
      "Epoch 0, batch 11799. Moving avg of loss: 2.76606339467\n",
      "Epoch 0, batch 11849. Moving avg of loss: 2.77423694995\n",
      "Epoch 0, batch 11899. Moving avg of loss: 2.77396617951\n",
      "Epoch 0, batch 11949. Moving avg of loss: 2.77021690995\n",
      "Epoch 0, batch 11999. Moving avg of loss: 2.78395600727\n",
      "Epoch 0, batch 12049. Moving avg of loss: 2.77764201926\n",
      "Epoch 0, batch 12099. Moving avg of loss: 2.76891514104\n",
      "Epoch 0, batch 12149. Moving avg of loss: 2.77565793742\n",
      "Epoch 0, batch 12199. Moving avg of loss: 2.77508892329\n",
      "Epoch 0, batch 12249. Moving avg of loss: 2.7612162688\n",
      "Epoch 0, batch 12299. Moving avg of loss: 2.75848227208\n",
      "Epoch 0, batch 12349. Moving avg of loss: 2.74945301426\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-639-97add62ae9a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mmoving_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mmoving_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.99\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmoving_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m.01\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python2.7/site-packages/mxnet/ndarray.pyc\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m    895\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels = nd.zeros((BATCH_SIZE, NEGATIVE_SAMPLES+1), ctx=ctx)\n",
    "labels[:,0] = 1\n",
    "start_time = time.time()\n",
    "epochs = 1\n",
    "for e in range(epochs):\n",
    "    moving_loss = 0.\n",
    "    for i, batch in enumerate(all_batches):\n",
    "        center_words = batch.data[0].as_in_context(ctx)\n",
    "        target_words = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            loss = model(center_words, target_words, labels)\n",
    "        loss.backward()\n",
    "        trainer.step(1, ignore_stale_grad=True)\n",
    "        \n",
    "        #  Keep a moving average of the losses\n",
    "        if (i == 0) and (e == 0):\n",
    "            moving_loss = loss.asnumpy().sum()\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * loss.asnumpy().sum()\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(\"Epoch %s, batch %s. Moving avg of loss: %s\" % (e, i, moving_loss))\n",
    "        if i > 15000:\n",
    "            break\n",
    "\n",
    "print(\"1 epoch took %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The input embedding finally has the word vectors we are interessted in. Normalizing all the word vectors and checking nearest neighbours. The training was stopped early so nearest neighbours for all words won't look reasonable. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = model.collect_params().keys()\n",
    "all_vecs = model.collect_params()[keys[0]].data().asnumpy()\n",
    "normalize(all_vecs, copy=False)\n",
    "\n",
    "def find_most_similar(word):\n",
    "    if word not in vocab:\n",
    "        print(\"Sorry word not found. Please try another one.\")\n",
    "    else:  \n",
    "        i1 = vocab[word]\n",
    "        prod = all_vecs.dot(all_vecs[i1])\n",
    "        i2 = (-prod).argsort()[1:10]\n",
    "        for i in i2:\n",
    "            print wid_to_word[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "south\n",
      "north\n",
      "west\n",
      "southern\n",
      "baltic\n",
      "coast\n",
      "central\n",
      "northern\n",
      "korea\n"
     ]
    }
   ],
   "source": [
    "find_most_similar(\"east\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "july\n",
      "april\n",
      "february\n",
      "march\n",
      "november\n",
      "october\n",
      "december\n",
      "august\n",
      "september\n"
     ]
    }
   ],
   "source": [
    "find_most_similar(\"january\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm\n",
      "ft\n",
      "kg\n",
      "metres\n",
      "km\n",
      "usd\n",
      "cm\n",
      "acres\n",
      "iso\n"
     ]
    }
   ],
   "source": [
    "find_most_similar(\"meters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
